{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_movies_all = pd.read_csv(\"data_analysis/movies.txt\", sep=',')\n",
    "df_cast_people = pd.read_csv(\"data_analysis/cast.txt\", sep=',')\n",
    "df_movie_companies = pd.read_csv(\"data_analysis/companies.txt\", sep=',')\n",
    "df_ratings = pd.read_csv(\"data_analysis/ratings.txt\", sep=',')\n",
    "df_users = pd.read_csv(\"data_analysis/users.txt\", sep=',')\n",
    "\n",
    "df_movies_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_cast_company=pd.read_csv('data_analysis/movies_cast_company.txt')\n",
    "movies_cast_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Merge the datasets\n",
    "user_movie_data = df_ratings.merge(movies_cast_company, on=\"movie_id_ml\")\n",
    "user_data = user_movie_data.merge(df_users, on=\"user_id\")\n",
    "\n",
    "# Select only numeric columns for aggregation\n",
    "numeric_user_data = user_data.select_dtypes(include=['number'])\n",
    "\n",
    "# Perform the aggregation (mean) on the numeric columns only\n",
    "user_features = numeric_user_data.groupby(\"user_id\").mean()\n",
    "# Only ratings are aggregated here\n",
    "\n",
    "# Select a random user\n",
    "random_user = user_features.sample(n=1)\n",
    "\n",
    "# Calculate average of non-zero ratings\n",
    "non_zero_counts = user_features[user_features > 0].count()\n",
    "\n",
    "\n",
    "non_zero_averages = user_features[user_features > 0].mean()  # Average rating of non-zero values\n",
    "\n",
    "#print(non_zero_counts)\n",
    "\n",
    "print(user_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Read IMDB title.basics file\n",
    "imdb_titles = pd.read_csv('data_analysis/title.basics.tsv', sep='\\t', low_memory=False, na_values=['\\\\N'])\n",
    "\n",
    "def fast_title_match(user_data, imdb_titles):\n",
    "    # Preprocess IMDB titles\n",
    "    imdb_titles = imdb_titles.copy()\n",
    "    \n",
    "    # Clean and convert columns\n",
    "    imdb_titles['primaryTitle'] = imdb_titles['primaryTitle'].astype(str)\n",
    "    imdb_titles['startYear'] = pd.to_numeric(imdb_titles['startYear'], errors='coerce')\n",
    "    \n",
    "    # Remove rows with NaN years or titles\n",
    "    imdb_titles = imdb_titles.dropna(subset=['primaryTitle', 'startYear', 'tconst'])\n",
    "    \n",
    "    # Create a mapping dictionary for faster lookup\n",
    "    title_dict = {\n",
    "        (row['primaryTitle'].lower(), int(row['startYear'])): row['tconst'] \n",
    "        for _, row in imdb_titles.iterrows()\n",
    "    }\n",
    "    \n",
    "    def find_match(title, year):\n",
    "        # Ensure title and year are in correct format\n",
    "        if pd.isna(title) or pd.isna(year):\n",
    "            return None\n",
    "        \n",
    "        title = str(title).lower()\n",
    "        try:\n",
    "            year = int(year)\n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "        \n",
    "        # Exact match first\n",
    "        exact_match = title_dict.get((title, year))\n",
    "        if exact_match:\n",
    "            return exact_match\n",
    "        \n",
    "        # Fuzzy matching with year range\n",
    "        for y in range(year-1, year+2):\n",
    "            candidates = [\n",
    "                (tconst, fuzz.ratio(title, t.lower())) \n",
    "                for (t, yr), tconst in title_dict.items() \n",
    "                if yr == y\n",
    "            ]\n",
    "            \n",
    "            if candidates:\n",
    "                best_match = max(candidates, key=lambda x: x[1])\n",
    "                return best_match[0] if best_match[1] > 80 else None\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # Vectorized matching\n",
    "    user_data = user_data.copy()\n",
    "    user_data['tconst'] = user_data.apply(\n",
    "        lambda row: find_match(row['title'], row['release']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return user_data\n",
    "\n",
    "# Apply the faster method\n",
    "merged_df_with_tconst = fast_title_match(user_data, imdb_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV\n",
    "#merged_df_with_tconst.to_csv('data_analysis/merged_dataset_with_tconst.csv', index=False)\n",
    "\n",
    "merged_df_with_tconst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read with dask\n",
    "principals = dd.read_csv('data_analysis/title.principals.tsv', \n",
    "                          sep='\\t', \n",
    "                          na_values=['\\\\N'])\n",
    "\n",
    "# Filter and compute\n",
    "directors = principals[principals['category'] == 'director'].compute()\n",
    "# Merge to get directors\n",
    "merged_with_directors = merged_df_with_tconst.merge(\n",
    "    directors[['tconst', 'nconst']], \n",
    "    on='tconst', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Read names to get director names\n",
    "names = pd.read_csv('data_analysis/name.basics.tsv', sep='\\t', low_memory=False, na_values=['\\\\N'])\n",
    "\n",
    "# Final merge to get director names\n",
    "final_df = merged_with_directors.merge(\n",
    "    names[['nconst', 'primaryName']], \n",
    "    on='nconst', \n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column and filter out NaNs\n",
    "final_df = final_df.rename(columns={'primaryName': 'director'})\n",
    "\n",
    "# Check total number of rows before filtering\n",
    "total_rows_before = len(final_df)\n",
    "\n",
    "# Filter out rows with NaN directors\n",
    "final_df_with_directors = final_df.dropna(subset=['director'])\n",
    "\n",
    "# Calculate and print statistics\n",
    "total_rows_after = len(final_df_with_directors)\n",
    "nan_count = total_rows_before - total_rows_after\n",
    "nan_percentage = (nan_count / total_rows_before) * 100\n",
    "\n",
    "print(f\"Total rows before filtering: {total_rows_before}\")\n",
    "print(f\"Total rows after filtering out NaN directors: {total_rows_after}\")\n",
    "print(f\"Number of rows removed due to NaN directors: {nan_count}\")\n",
    "print(f\"Percentage of rows with missing directors: {nan_percentage:.2f}%\")\n",
    "\n",
    "# Optional: Save the filtered DataFrame\n",
    "#final_df_with_directors.to_csv('final_dataset_with_directors.csv', index=False)\n",
    "\n",
    "# Preview the first few rows of directors\n",
    "print(\"\\nSample of directors:\")\n",
    "print(final_df_with_directors[['title', 'director']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.read_csv(\"data_analysis/final_dataset.csv\")\n",
    "\n",
    "import random\n",
    "\n",
    "# Select only numeric columns for aggregation\n",
    "numeric_user_data = user_data.select_dtypes(include=['number'])\n",
    "\n",
    "# Perform the aggregation (mean) on the numeric columns only\n",
    "user_features = numeric_user_data.groupby(\"user_id\").mean()\n",
    "# Only ratings are aggregated here\n",
    "\n",
    "# Select a random user\n",
    "random_user = user_features.sample(n=1)\n",
    "\n",
    "# Calculate average of non-zero ratings\n",
    "non_zero_counts = user_features[user_features > 0].count()\n",
    "\n",
    "\n",
    "non_zero_averages = user_features[user_features > 0].mean()  # Average rating of non-zero values\n",
    "\n",
    "#print(non_zero_counts)\n",
    "\n",
    "print(user_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "\n",
    "# Function to clean and normalize keywords\n",
    "def clean_keywords(keyword):\n",
    "    if pd.isnull(keyword):\n",
    "        return \"\"\n",
    "    # Convert to lowercase and remove special characters\n",
    "    cleaned = keywords.lower()\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9, ]', '', cleaned)  # Keep only alphanumerics and commas\n",
    "    return cleaned\n",
    "\n",
    "# Function to extract 'cast_name' from the 'cast' column\n",
    "def extract_cast_names(cast_data):\n",
    "    try:\n",
    "        # Convert the stringified list of dictionaries to Python objects\n",
    "        cast_list = ast.literal_eval(cast_data)\n",
    "        formatted_names = []\n",
    "        for entry in cast_list:\n",
    "            if \"cast_name\" in entry:\n",
    "                # Split the name into surname and first name\n",
    "                cast_name = entry[\"cast_name\"]\n",
    "                # Split by comma (i.e., \"surname, first_name\")\n",
    "                name_parts = cast_name.split(\", \")\n",
    "                if len(name_parts) == 2:\n",
    "                    surname, first_name = name_parts\n",
    "                    # Format as \"surname first_name\"\n",
    "                    formatted_names.append(f\"{surname} {first_name}\")\n",
    "        return \", \".join(formatted_names)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return \"\"\n",
    "\n",
    "# Function to extract 'name' from the 'company' column\n",
    "def extract_company_names(company_data):\n",
    "    try:\n",
    "        # Convert the stringified list of dictionaries to Python objects\n",
    "        company_list = ast.literal_eval(company_data)\n",
    "        # Extract 'name' from each dictionary in the list\n",
    "        return \", \".join([entry[\"name\"] for entry in company_list if \"name\" in entry])\n",
    "    except (ValueError, SyntaxError):\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter\n",
    "\n",
    "# Merge the datasets\n",
    "user_movie_data = df_ratings.merge(movies_cast_company, on=\"movie_id_ml\")\n",
    "user_data = user_movie_data.merge(df_users, on=\"user_id\")\n",
    "\n",
    "# Step 1: Clean keywords, cast, and company data\n",
    "user_data['cleaned_keywords'] = user_data['keyword'].fillna('').str.strip().str.lower()\n",
    "user_data['cast'] = user_data['cast'].fillna('').str.strip().str.lower()\n",
    "user_data['company'] = user_data['company'].fillna('').str.strip().str.lower()\n",
    "\n",
    "user_data['cast_names'] = user_data['cast'].apply(extract_cast_names)\n",
    "user_data['company_names'] = user_data['company'].apply(extract_company_names)\n",
    "\n",
    "# Analyze keywords\n",
    "all_keywords = user_data['cleaned_keywords'].str.split(',').explode()\n",
    "keyword_counts = Counter(all_keywords)\n",
    "top_keywords = keyword_counts.most_common(10)\n",
    "print(\"Top Keywords:\", top_keywords)\n",
    "\n",
    "# Analyze cast\n",
    "all_cast = user_data['cast_names'].str.split(',').explode()\n",
    "cast_counts = Counter(all_cast)\n",
    "top_cast = cast_counts.most_common(10)\n",
    "print(\"Top Cast:\", top_cast)\n",
    "\n",
    "# Analyze companies\n",
    "all_comp = user_data['company_names'].str.split(',').explode()\n",
    "comp_counts = Counter(all_comp)\n",
    "top_comp = comp_counts.most_common(10)\n",
    "print(\"Top Companies:\", top_comp)\n",
    "\n",
    "# Step 2: Use CountVectorizer with top keywords, cast, and company names for encoding\n",
    "top_k = 30  # Adjust based on memory constraints\n",
    "most_common_keywords = [keyword for keyword, _ in keyword_counts.most_common(top_k)]\n",
    "most_common_cast = [cast for cast, _ in cast_counts.most_common(top_k)]\n",
    "most_common_companies = [company for company, _ in comp_counts.most_common(top_k)]\n",
    "\n",
    "# Vectorizing columns with CountVectorizer\n",
    "vectorizer_keywords = CountVectorizer(tokenizer=lambda x: x.split(','), lowercase=True, vocabulary=most_common_keywords)\n",
    "vectorizer_cast = CountVectorizer(tokenizer=lambda x: x.split(','), lowercase=True, vocabulary=most_common_cast)\n",
    "vectorizer_companies = CountVectorizer(tokenizer=lambda x: x.split(','), lowercase=True, vocabulary=most_common_companies)\n",
    "\n",
    "# Transform the columns\n",
    "keyword_matrix = vectorizer_keywords.fit_transform(user_data['cleaned_keywords'])\n",
    "cast_matrix = vectorizer_cast.fit_transform(user_data['cast_names'])\n",
    "company_matrix = vectorizer_companies.fit_transform(user_data['company_names'])\n",
    "\n",
    "# Convert sparse matrices to DataFrames\n",
    "keyword_df = pd.DataFrame(keyword_matrix.toarray(), columns=vectorizer_keywords.get_feature_names_out())\n",
    "cast_df = pd.DataFrame(cast_matrix.toarray(), columns=vectorizer_cast.get_feature_names_out())\n",
    "company_df = pd.DataFrame(company_matrix.toarray(), columns=vectorizer_companies.get_feature_names_out())\n",
    "\n",
    "# Concatenate encoded data with user_data\n",
    "user_data = pd.concat([user_data.reset_index(drop=True), keyword_df, cast_df, company_df], axis=1)\n",
    "\n",
    "# Remove duplicate columns\n",
    "user_data = user_data.loc[:, ~user_data.columns.duplicated()]\n",
    "\n",
    "# Step 3: Handle numeric columns and scaling\n",
    "numeric_user_data = user_data.select_dtypes(include=['number'])\n",
    "scaler = MinMaxScaler()\n",
    "numeric_user_data_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(numeric_user_data),\n",
    "    columns=numeric_user_data.columns\n",
    ")\n",
    "\n",
    "# Aggregate user data by mean\n",
    "user_features = numeric_user_data.groupby(\"user_id\").mean()\n",
    "\n",
    "# Check correlation\n",
    "corr = user_features.corr()\n",
    "print(corr[\"rating\"].sort_values(ascending=False))\n",
    "\n",
    "# Step 4: Prepare data for model training\n",
    "X = user_features.drop(\"rating\", axis=1)  # Drop target column\n",
    "y = user_features[\"rating\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance scores\n",
    "feature_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "\n",
    "# Drop features with zero importance\n",
    "non_zero_features = feature_importances[feature_importances > 0].index\n",
    "X = X[non_zero_features]\n",
    "X_train = X_train[non_zero_features]\n",
    "X_test = X_test[non_zero_features]\n",
    "\n",
    "# Retrain the model with reduced features\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Plot the top_n most important features\n",
    "top_n = 100\n",
    "top_features = feature_importances.nlargest(top_n)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "top_features.plot(kind=\"bar\")\n",
    "plt.title(f\"Top {top_n} Feature Importances\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optionally, print the most important features\n",
    "print(\"Most important features:\")\n",
    "print(top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Calculate the number of non-zero entries for each feature\n",
    "feature_non_zero_count = (user_data != 0).sum(axis=0)\n",
    "\n",
    "# Print out the number of rows in which each feature is found\n",
    "print(\"Number of non-zero rows for each feature:\")\n",
    "print(feature_non_zero_count)\n",
    "\n",
    "# Step 2: Plot a histogram of the number of rows each feature is found in\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(feature_non_zero_count, bins=50, edgecolor='black')\n",
    "plt.title('Histogram of the Number of Rows in Which Each Feature is Found')\n",
    "plt.xlabel('Number of Non-Zero Rows per Feature')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log scaling\n",
    "feature_non_zero_count = (user_data != 0).sum(axis=0)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(feature_non_zero_count, bins=50, edgecolor='black', log=True)\n",
    "plt.title('Histogram of Non-Zero Feature Counts')\n",
    "plt.xlabel('Number of Non-Zero Rows per Feature')\n",
    "plt.ylabel('Log Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"user_data\": user_data,\n",
    "    \"movies_cast_company\": movies_cast_company,\n",
    "    \"df_movies_all\": df_movies_all,\n",
    "    \"df_cast_people\": df_cast_people,\n",
    "    \"df_movie_companies\": df_movie_companies,\n",
    "    \"df_ratings\": df_ratings,\n",
    "    \"df_users\": df_users,\n",
    "}\n",
    "for df_name, df in datasets.items():\n",
    "    print(f\"Dataset: {df_name}\")\n",
    "    print(df.info())\n",
    "    print(df.describe())\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adding director"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Load your dataset (ensure it has 'title' and 'imdb_id' columns)\n",
    "data_path = 'data_analysis/movies_cast_company.txt'  # Replace with your dataset file path\n",
    "links = pd.read_csv('data_analysis/links.csv')  # MovieLens links dataset\n",
    "\n",
    "# Merge to map movie_id_ml to IMDb IDs\n",
    "movies_with_imdb = movies.merge(links, left_on='movie_id_ml', right_on='movieId', how='left')\n",
    "\n",
    "# Create the IMDb ID string required for IMDb URLs\n",
    "movies_with_imdb['imdb_id'] = movies_with_imdb['imdbId'].apply(lambda x: f\"tt{int(x):07d}\" if not pd.isna(x) else None)\n",
    "\n",
    "# Save the updated dataset\n",
    "movies_with_imdb.to_csv('movies_with_imdb_ids.csv', index=False)\n",
    "print(\"Saved dataset with IMDb IDs as 'movies_with_imdb_ids.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def fetch_director_from_imdb(imdb_id):\n",
    "    \"\"\"\n",
    "    Fetch the director's name for a movie using its IMDb ID.\n",
    "    \"\"\"\n",
    "    url = f\"https://www.imdb.com/title/{imdb_id}/\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch IMDb page for ID {imdb_id}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Locate the director(s) in the IMDb page\n",
    "    director_section = soup.find_all('div', {'class': 'credit_summary_item'})\n",
    "    directors = []\n",
    "    \n",
    "    for section in director_section:\n",
    "        if 'Director' in section.get_text():\n",
    "            director_links = section.find_all('a')\n",
    "            directors = [a.text.strip() for a in director_links]\n",
    "            break\n",
    "    \n",
    "    if directors:\n",
    "        return ', '.join(directors)\n",
    "    \n",
    "    return None  # Director not found\n",
    "\n",
    "# Load dataset and merge with IMDb IDs\n",
    "movies = pd.read_csv('movies_with_imdb_ids.csv')  # Dataset with IMDb IDs\n",
    "\n",
    "# Filter out movies without IMDb IDs\n",
    "movies = movies[movies['imdb_id'].notna()]\n",
    "\n",
    "# Fetch directors using the updated scraping function\n",
    "directors = []\n",
    "for index, row in movies.iterrows():\n",
    "    imdb_id = row['imdb_id']\n",
    "    print(f\"Fetching director for IMDb ID: {imdb_id} ({row['title']})\")\n",
    "    director = fetch_director_from_imdb(imdb_id)\n",
    "    directors.append(director)\n",
    "    time.sleep(2)  # Increase delay to reduce rate limiting\n",
    "\n",
    "movies['director'] = directors\n",
    "\n",
    "# Save the updated dataset\n",
    "movies.to_csv('movies_with_directors.csv', index=False)\n",
    "print(\"Directors added and dataset saved to 'movies_with_directors.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
